{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Deep Learning\n",
    "# Part 3 - Multiple Layer Neural Networks\n",
    "### Delta rule for learning, Gradient Decent and backpropogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import utils_plot as uplot\n",
    "import matplotlib.gridspec as gridspec\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Networks without hidden units are very limited in what they can learn to model. What we need is multiple layers of non-linear hidden units. <br>\n",
    "The chalange is: **how to train such netorks ?**<br>\n",
    "We need a way to update all the weights not just the last layer like in a perceptron. It is a hard problem, and it took the researchers 20 years to find the right method. <br>\n",
    "\n",
    "\n",
    "![title](training_model.png)\n",
    "Any hidden unit, can affect many other units, and affect the results, in many ways. So we use a method to combine all those effects, and focus on investigating the efect, the weight have, on the overall error.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of the learning is to minimize the error summed (or meaned) over all training cases. <br>\n",
    "To achive that,we need a **measure** of that error. For simplicity, we will use the square difference between the target output and the actual output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Squared loss: a popular loss function\n",
    "The squared loss (also known as L2 loss), is the squered distance between the prediction and the true lable. <br>\n",
    "Lets mark:\n",
    "* $x\\, -\\, The\\, input$\n",
    "* $\\hat{y}\\, or \\,y'\\, - \\,The\\, true\\, lables$<br>\n",
    "* $y = predictions(x) = f(x)\\; Model's\\,  predictions$<br>\n",
    "\n",
    "The squared loss for a single example is: ${(\\hat{y} - y)}^2$<br>\n",
    "\n",
    " \n",
    "#### Mean square error (MSE) \n",
    "is the average squared loss per example over the whole dataset. \n",
    "To calculate MSE, sum up all the squared\n",
    "losses for individual examples and then divide by the number of examples:\n",
    "    $$ MSE = \\frac{1}{N} \\sum_{(x,y)\\in D} (y - f(x))^2 $$\n",
    "> MSE is popular in machine learning, mainly from historical reasons, since all the math was initially calculated with MSE. But it is not neccesery the best. For different problems, there are other practical loss functions that performs better.\n",
    "\n",
    "#### The goal of training a model is to find a set of weights that produce a minimal loss, on average, across all examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the minimum of the loss function, we will use An iterative method. <br>\n",
    "Iterative methods, are usually less efficient, but much easier to generalize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Decent - Minimize loss function iterativly \n",
    "Intuition: Imagine you stand in a canyon, or a bowl, and you want to get to the lowest point. If you walk in small steps towards the stipest part, this will get you to the lowest point, if the canyon (or the bowl) is convex.\n",
    "![title](dune11.jpg)\n",
    "### Delta rule for learning\n",
    "This intuition is defined as the delta rule.\n",
    "$$ \\Delta{w_{i}} = \\alpha*(derivative\\, of\\, the \\,loss\\, function)$$\n",
    "\n",
    "* $\\alpha$ - learning rate, is the (small) size of the step.\n",
    "\n",
    "The weights update will be:\n",
    "$$ W_{new} = W_{old} - \\Delta{w_{i}}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But how do we implement the delta rule for a multi layer network ?\n",
    "This implementation is called backpropogation. Backpropagation, is an abbrevation for “backward propagation of errors”, is a mechanism used to update the weights using gradient descent. It calculates the gradient of the error function with respect to the neural network’s weights. The calculation proceeds backwards through the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "Back propogation,is a method to propogatate the error, back to the weights, and update them in away that will minimize the error.<br>\n",
    "Math is simple algebra, but there are some indexing to follow. Since most of us will not code the backpropopgation manually, The coded is illustrative, specific and not too general. Still you can play with the coded example and test different parameters and data sets.<br>\n",
    "To implement backpropogation example, Lets start with our Logic OR example.\n",
    "![title](mlp1j11.jpg)\n",
    "### Step 1. Initialize network\n",
    "For simplicity, we choose all activation functions to be Sigmoid function. For clarity, We will cal the weighted input sum to a neuron: net, and the Sigmoid applied on the this sum will be called: out.\n",
    "![title](neuron_in_out.JPG)\n",
    "Build and Initialize network with random weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a network\n",
    "n_inputs = 3\n",
    "n_hidden = 2\n",
    "\n",
    "def initialize_network():\n",
    "    \n",
    "    network = {'n_inputs':n_inputs,'n_hidden':n_hidden,\n",
    "               'n_outputs':1,\n",
    "               'hidden_layer_weights':\\\n",
    "               [[np.random.random() for i in range(n_inputs)] for i in range(n_hidden)],\n",
    "              'output_layer_weights':\\\n",
    "              [np.random.random() for i in range(n_hidden)],\n",
    "               \n",
    "              'output_layer_net':0.,\n",
    "              'hidden_net':\\\n",
    "               [0. for i in range(n_hidden)],\n",
    "               'hidden_out':\\\n",
    "               [0. for i in range(n_hidden)],\n",
    "               'output_layer_out':0.\n",
    "              }\n",
    "    \n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_network(network):\n",
    "    print('hidden_layer_weights:')\n",
    "    \n",
    "    for i in range(n_hidden):\n",
    "        print('h{} weights:{}'.format(i, network['hidden_layer_weights'][i]))\n",
    "        print('h{} net:{:.4f}'.format(i, network['hidden_net'][i]))\n",
    "        print('h{} out:{:.4f}'.format(i, network['hidden_out'][i]))\n",
    "    print('output_layer_weights: {}'.format(network['output_layer_weights']))\n",
    "    print('output layer net: {:.4f}'.format(network['output_layer_net']))\n",
    "    print('Output: {:.4f}'.format(network['output_layer_out']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_layer_weights:\n",
      "h0 weights:[0.5488135039273248, 0.7151893663724195, 0.6027633760716439]\n",
      "h0 net:0.0000\n",
      "h0 out:0.0000\n",
      "h1 weights:[0.5448831829968969, 0.4236547993389047, 0.6458941130666561]\n",
      "h1 net:0.0000\n",
      "h1 out:0.0000\n",
      "output_layer_weights: [0.4375872112626925, 0.8917730007820798]\n",
      "output layer net: 0.0000\n",
      "Output: 0.0000\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "network = initialize_network()\n",
    "print_network(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass: propagate input to a network output\n",
    "def forward_propagate(network, inputs):\n",
    "    new_inputs = []\n",
    "    for i in range(network['n_hidden']):\n",
    "        #print('i',i, network['hidden_layer_weights'][i])\n",
    "        #print(inputs)\n",
    "        network['hidden_net'][i] = np.dot(network['hidden_layer_weights'][i],\n",
    "                                                inputs)\n",
    "        network['hidden_out'][i] = sigmoid(network['hidden_net'][i])\n",
    "    \n",
    "   \n",
    "    net = np.dot(network['output_layer_weights'], \n",
    "                 [network['hidden_out'][i] for i in range(n_hidden)])\n",
    "    out = sigmoid(net)\n",
    "    network['output_layer_net'] = net\n",
    "    network['output_layer_out'] = out\n",
    "    \n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = forward_propagate(network, [1, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_layer_weights:\n",
      "h0 weights:[0.5488135039273248, 0.7151893663724195, 0.6027633760716439]\n",
      "h0 net:1.1516\n",
      "h0 out:0.7598\n",
      "h1 weights:[0.5448831829968969, 0.4236547993389047, 0.6458941130666561]\n",
      "h1 net:1.1908\n",
      "h1 out:0.7669\n",
      "output_layer_weights: [0.4375872112626925, 0.8917730007820798]\n",
      "output layer net: 1.0164\n",
      "Output: 0.7343\n"
     ]
    }
   ],
   "source": [
    "print_network(network)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Backpropogation\n",
    "To apply the delta rule for learning, we need todefine a loss function. Lets use the simple known squere error:\n",
    "$$E_{total} = \\frac{1}{2}(target - output)^{2}$$\n",
    "To update weights, according to delta rule, we need to calculate the \"influence\"of each weight onthe target. Mathematicly, we need to calculate the partial derivativeof the error, with respect to each weight. <br>\n",
    "Lets mark the weights in our network , and rename input layer to $I_i$:\n",
    "![title](mlp_2hidden_1ou_weightst.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backpropogationg error for output layer\n",
    "Consider $w_1$, We want to know how much a change in $w_7$ affects the total error, aka, $\\frac{\\partial E_{total}}{\\partial w_{1}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using chane rule, we will get:\n",
    "$$\\frac{\\partial E_{total}}{\\partial w_{1}} = \\frac{\\partial E_{total}}{\\partial out_o} * \n",
    "\\frac{\\partial out_o}{\\partial net_{o}} *\n",
    "\\frac{\\partial net_{o}}{\\partial w_{1}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$E_{total}=(target - output)^{2 }$$\n",
    "$$output = out_o$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial E_{total}}{\\partial out_o} = 2 * \\frac{1}{2}(target - out_o)^{2 - 1} * -1 =(out_o-target)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose Sigmoid activation, wich derivative is:\n",
    "$$\\frac{d}{dx}Sigmoid(x) = Sigmoid(x)(1 - Sigmoid(x))$$\n",
    "* detailes https://beckernick.github.io/sigmoid-derivative-neural-network/\n",
    "\n",
    "And since:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "$$output = out_o = \\frac{1}{1+e^{-net_{o}}}$$\n",
    "We get:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial out_o}{\\partial net_{o}} = out_o*(1-out_o)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$net_{o} = out_{h0}*w_0 + out_{h1}*w_1$$    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$out_{h0}$ ho has no dependence in $w_{1}$, So:\n",
    "$$\\frac{\\partial net_{o}}{\\partial w_{1}} = out_{h1}$$    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Putting it all together, we get:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial E_{total}}{\\partial w_{1}} = \\frac{\\partial E_{total}}{\\partial out_o} * \n",
    "\\frac{\\partial out_o}{\\partial net_{o}} *\n",
    "\\frac{\\partial net_{o}}{\\partial w_{1}} =  -(target-out_o)*\n",
    "out_o*(1-out_o)*\n",
    "out_{h1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So according to delta rule, to decrease error, we update $w_1$:\n",
    "$$w_1^{new} = w_1 + \\Delta* (target-out_o)*  out_o*(1-out_o)*out_{h1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the same, we can calculate update for $w_0$:\n",
    "$$w_0^{new} = w_0 + \\Delta* (target-out_o)*  out_o*(1-out_o)*out_{ho}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets mark:\n",
    "$$ \\delta_{o} = out_o*(1-out_o)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get:\n",
    "$$w_0^{new} = w_0 + \\Delta* \\delta_{o}*out_{ho}*(target-output)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$w_1^{new} = w_1 + \\Delta* \\delta_{o}*out_{h1}*(target-output)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backpropogationg error for hidden layer\n",
    "In a similar way, lets calculate: $\\frac{\\partial E_{total}}{\\partial w_{12}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial E_{total}}{\\partial w_{12}} = [\\frac{\\partial E_{total}}{\\partial out_{o}} * \n",
    "\\frac{\\partial out_{o}}{\\partial net_{o}}] * \n",
    "\\frac{\\partial net_{o}}{\\partial out_{h1}} * \n",
    "\\frac{\\partial out_{h1}}{\\partial net_{h1}} *\n",
    "\\frac{\\partial net_{h1}}{\\partial w_{12}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial E_{total}}{\\partial w_{12}} = -[\\delta_{out}*out_{h1}*(target-output)]*\\frac{\\partial net_{o}}{\\partial out_{h1}} * \n",
    "\\frac{\\partial out_{h1}}{\\partial net_{h1}} *\n",
    "\\frac{\\partial net_{h1}}{\\partial w_{12}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$net_{o} = out_{h0}*w_0 + out_{h1}*w_1$$   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial net_{o}}{\\partial out_{h1}} = w_1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial out_{h1}}{\\partial net_{h1}} = out_{h1}*(1-out_{h1})=\\delta_{h1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial net_{h1}}{\\partial w_{12}} = I_{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting it all together, we get:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial E_{total}}{\\partial w_{11}} = -(target-output)*\\delta_{out}*out_{h1}*w_1*\\delta_{h1}*I_1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial E_{total}}{\\partial w_{10}} = -(target-output)*\\delta_{out}*out_{h1}*w_1*\\delta_{h1}*I_0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial E_{total}}{\\partial w_{00}} = -(target-output)*\\delta_{out}*out_{h0}*w_0*\\delta_{h0}*I_0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write, general update rule for hidden layerofour network:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial E_{total}}{\\partial w_{ij}} = -(target-output)*\\delta_{o}*out_{hi}*w_i*\\delta_{hi}*I_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, update rule for hidden layer weights, will be:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$w^{new}_{ij} = w_{ij} + \\Delta *(target-output)*\\delta_{o}*out_{hi}*w_i*\\delta_{hi}*I_j$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta(x):    \n",
    "    return x * (1 - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backpropagate error and update weights\n",
    "def backward_propagate_error(network, inputs, output, target, lr = 0.5):\n",
    "    \n",
    "    delta_o = delta(output)\n",
    "\n",
    "    for i in range(n_hidden):\n",
    "\n",
    "        # lets mark: update_i = delta_o*out_hi*(target - output)\n",
    "        update_i = delta_o * network['hidden_out'][i] * (target - output)\n",
    "               \n",
    "        # update hidden layer weights        \n",
    "        # We get: w_new_ij = w_ij + lr*update_i*w_i*delta_hi*Ij\n",
    "        for j in range(n_inputs):\n",
    "            w_i = network['output_layer_weights'][i]\n",
    "            out_hi = network['hidden_out'][i]\n",
    "            update_ij = update_i*w_i*delta(out_hi)*inputs[j]\n",
    "            network['hidden_layer_weights'][i][j] += lr*update_ij\n",
    "       \n",
    "        # update output layer weights\n",
    "        # w_new_i = w_i + lr*delta_o*out_hi*(target - output)\n",
    "        # w_new_i = w_i + lr*update_i\n",
    "        network['output_layer_weights'][i] +=  lr * update_i    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some Test of back propogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_layer_weights:\n",
      "h0 weights:[0.5488135039273248, 0.7151893663724195, 0.6027633760716439]\n",
      "h0 net:0.5488\n",
      "h0 out:0.6339\n",
      "h1 weights:[0.5448831829968969, 0.4236547993389047, 0.6458941130666561]\n",
      "h1 net:0.5449\n",
      "h1 out:0.6329\n",
      "output_layer_weights: [0.4375872112626925, 0.8917730007820798]\n",
      "output layer net: 0.8418\n",
      "Output: 0.6988\n",
      "hidden_layer_weights:\n",
      "h0 weights:[0.5440795950731664, 0.7151893663724195, 0.6027633760716439]\n",
      "h0 net:0.5488\n",
      "h0 out:0.6339\n",
      "h1 weights:[0.5352395813873537, 0.4236547993389047, 0.6458941130666561]\n",
      "h1 net:0.5449\n",
      "h1 out:0.6329\n",
      "output_layer_weights: [0.39097337104818664, 0.8452262752812989]\n",
      "output layer net: 0.8418\n",
      "Output: 0.6988\n"
     ]
    }
   ],
   "source": [
    "inputs = [1,0,0]\n",
    "target = 0\n",
    "output = forward_propagate(network, inputs)\n",
    "print_network(network)\n",
    "backward_propagate_error(network, inputs, output, target)\n",
    "print_network(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(network, train, targets, l_rate, n_epoch, verbose=1):\n",
    "    \n",
    "    epoch_error = []\n",
    "    #print('in train_error')\n",
    "    for epoch in range(n_epoch):\n",
    "        sum_error = 0\n",
    "        for inputs,target in zip(train,targets):\n",
    "            #print('inputs', inputs)\n",
    "            #print_network(network)\n",
    "            output = forward_propagate(network, inputs)\n",
    "            # update the weights according to error in the output\n",
    "            sum_error += np.sum((target-output)**2)\n",
    "            backward_propagate_error(network, inputs, output, target)\n",
    "        epoch_error.append(sum_error)\n",
    "        if (verbose):\n",
    "            print('>epoch=%d,  error=%.3f' % (epoch, sum_error))\n",
    "    return epoch_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_layer_weights:\n",
      "h0 weights:[0.5440795950731664, 0.7151893663724195, 0.6027633760716439]\n",
      "h0 net:1.1468\n",
      "h0 out:0.7589\n",
      "h1 weights:[0.5352395813873537, 0.4236547993389047, 0.6458941130666561]\n",
      "h1 net:1.1811\n",
      "h1 out:0.7652\n",
      "output_layer_weights: [0.39097337104818664, 0.8452262752812989]\n",
      "output layer net: 0.9434\n",
      "Output: 0.7198\n"
     ]
    }
   ],
   "source": [
    "output = forward_propagate(network, [1, 0, 1])\n",
    "print_network(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets train the network on Logic OR \n",
    "* Inputs are $[bias=1, X_0, X_1]$\n",
    "* Targets are:  $X_0 \\vee X_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">epoch=0,  error=0.706\n",
      ">epoch=1,  error=0.704\n",
      ">epoch=2,  error=0.702\n",
      ">epoch=3,  error=0.700\n",
      ">epoch=4,  error=0.699\n",
      ">epoch=5,  error=0.698\n",
      ">epoch=6,  error=0.697\n",
      ">epoch=7,  error=0.696\n",
      ">epoch=8,  error=0.696\n",
      ">epoch=9,  error=0.695\n",
      ">epoch=10,  error=0.694\n",
      ">epoch=11,  error=0.694\n",
      ">epoch=12,  error=0.693\n",
      ">epoch=13,  error=0.693\n",
      ">epoch=14,  error=0.692\n",
      ">epoch=15,  error=0.692\n",
      ">epoch=16,  error=0.691\n",
      ">epoch=17,  error=0.691\n",
      ">epoch=18,  error=0.690\n",
      ">epoch=19,  error=0.690\n",
      ">epoch=20,  error=0.689\n",
      ">epoch=21,  error=0.689\n",
      ">epoch=22,  error=0.688\n",
      ">epoch=23,  error=0.688\n",
      ">epoch=24,  error=0.688\n",
      ">epoch=25,  error=0.687\n",
      ">epoch=26,  error=0.687\n",
      ">epoch=27,  error=0.686\n",
      ">epoch=28,  error=0.686\n",
      ">epoch=29,  error=0.685\n",
      ">epoch=30,  error=0.685\n",
      ">epoch=31,  error=0.685\n",
      ">epoch=32,  error=0.684\n",
      ">epoch=33,  error=0.684\n",
      ">epoch=34,  error=0.683\n",
      ">epoch=35,  error=0.683\n",
      ">epoch=36,  error=0.682\n",
      ">epoch=37,  error=0.682\n",
      ">epoch=38,  error=0.681\n",
      ">epoch=39,  error=0.681\n",
      ">epoch=40,  error=0.681\n",
      ">epoch=41,  error=0.680\n",
      ">epoch=42,  error=0.680\n",
      ">epoch=43,  error=0.679\n",
      ">epoch=44,  error=0.679\n",
      ">epoch=45,  error=0.678\n",
      ">epoch=46,  error=0.678\n",
      ">epoch=47,  error=0.678\n",
      ">epoch=48,  error=0.677\n",
      ">epoch=49,  error=0.677\n",
      ">epoch=50,  error=0.676\n",
      ">epoch=51,  error=0.676\n",
      ">epoch=52,  error=0.675\n",
      ">epoch=53,  error=0.675\n",
      ">epoch=54,  error=0.674\n",
      ">epoch=55,  error=0.674\n",
      ">epoch=56,  error=0.674\n",
      ">epoch=57,  error=0.673\n",
      ">epoch=58,  error=0.673\n",
      ">epoch=59,  error=0.672\n",
      ">epoch=60,  error=0.672\n",
      ">epoch=61,  error=0.671\n",
      ">epoch=62,  error=0.671\n",
      ">epoch=63,  error=0.670\n",
      ">epoch=64,  error=0.670\n",
      ">epoch=65,  error=0.669\n",
      ">epoch=66,  error=0.669\n",
      ">epoch=67,  error=0.669\n",
      ">epoch=68,  error=0.668\n",
      ">epoch=69,  error=0.668\n",
      ">epoch=70,  error=0.667\n",
      ">epoch=71,  error=0.667\n",
      ">epoch=72,  error=0.666\n",
      ">epoch=73,  error=0.666\n",
      ">epoch=74,  error=0.665\n",
      ">epoch=75,  error=0.665\n",
      ">epoch=76,  error=0.665\n",
      ">epoch=77,  error=0.664\n",
      ">epoch=78,  error=0.664\n",
      ">epoch=79,  error=0.663\n",
      ">epoch=80,  error=0.663\n",
      ">epoch=81,  error=0.662\n",
      ">epoch=82,  error=0.662\n",
      ">epoch=83,  error=0.661\n",
      ">epoch=84,  error=0.661\n",
      ">epoch=85,  error=0.660\n",
      ">epoch=86,  error=0.660\n",
      ">epoch=87,  error=0.659\n",
      ">epoch=88,  error=0.659\n",
      ">epoch=89,  error=0.659\n",
      ">epoch=90,  error=0.658\n",
      ">epoch=91,  error=0.658\n",
      ">epoch=92,  error=0.657\n",
      ">epoch=93,  error=0.657\n",
      ">epoch=94,  error=0.656\n",
      ">epoch=95,  error=0.656\n",
      ">epoch=96,  error=0.655\n",
      ">epoch=97,  error=0.655\n",
      ">epoch=98,  error=0.654\n",
      ">epoch=99,  error=0.654\n",
      ">epoch=100,  error=0.653\n",
      ">epoch=101,  error=0.653\n",
      ">epoch=102,  error=0.652\n",
      ">epoch=103,  error=0.652\n",
      ">epoch=104,  error=0.652\n",
      ">epoch=105,  error=0.651\n",
      ">epoch=106,  error=0.651\n",
      ">epoch=107,  error=0.650\n",
      ">epoch=108,  error=0.650\n",
      ">epoch=109,  error=0.649\n",
      ">epoch=110,  error=0.649\n",
      ">epoch=111,  error=0.648\n",
      ">epoch=112,  error=0.648\n",
      ">epoch=113,  error=0.647\n",
      ">epoch=114,  error=0.647\n",
      ">epoch=115,  error=0.646\n",
      ">epoch=116,  error=0.646\n",
      ">epoch=117,  error=0.645\n",
      ">epoch=118,  error=0.645\n",
      ">epoch=119,  error=0.644\n",
      ">epoch=120,  error=0.644\n",
      ">epoch=121,  error=0.643\n",
      ">epoch=122,  error=0.643\n",
      ">epoch=123,  error=0.643\n",
      ">epoch=124,  error=0.642\n",
      ">epoch=125,  error=0.642\n",
      ">epoch=126,  error=0.641\n",
      ">epoch=127,  error=0.641\n",
      ">epoch=128,  error=0.640\n",
      ">epoch=129,  error=0.640\n",
      ">epoch=130,  error=0.639\n",
      ">epoch=131,  error=0.639\n",
      ">epoch=132,  error=0.638\n",
      ">epoch=133,  error=0.638\n",
      ">epoch=134,  error=0.637\n",
      ">epoch=135,  error=0.637\n",
      ">epoch=136,  error=0.636\n",
      ">epoch=137,  error=0.636\n",
      ">epoch=138,  error=0.635\n",
      ">epoch=139,  error=0.635\n",
      ">epoch=140,  error=0.634\n",
      ">epoch=141,  error=0.634\n",
      ">epoch=142,  error=0.633\n",
      ">epoch=143,  error=0.633\n",
      ">epoch=144,  error=0.632\n",
      ">epoch=145,  error=0.632\n",
      ">epoch=146,  error=0.631\n",
      ">epoch=147,  error=0.631\n",
      ">epoch=148,  error=0.630\n",
      ">epoch=149,  error=0.630\n",
      ">epoch=150,  error=0.629\n",
      ">epoch=151,  error=0.629\n",
      ">epoch=152,  error=0.628\n",
      ">epoch=153,  error=0.628\n",
      ">epoch=154,  error=0.627\n",
      ">epoch=155,  error=0.627\n",
      ">epoch=156,  error=0.626\n",
      ">epoch=157,  error=0.626\n",
      ">epoch=158,  error=0.625\n",
      ">epoch=159,  error=0.625\n",
      ">epoch=160,  error=0.624\n",
      ">epoch=161,  error=0.624\n",
      ">epoch=162,  error=0.623\n",
      ">epoch=163,  error=0.623\n",
      ">epoch=164,  error=0.622\n",
      ">epoch=165,  error=0.622\n",
      ">epoch=166,  error=0.621\n",
      ">epoch=167,  error=0.621\n",
      ">epoch=168,  error=0.620\n",
      ">epoch=169,  error=0.620\n",
      ">epoch=170,  error=0.619\n",
      ">epoch=171,  error=0.619\n",
      ">epoch=172,  error=0.618\n",
      ">epoch=173,  error=0.618\n",
      ">epoch=174,  error=0.617\n",
      ">epoch=175,  error=0.617\n",
      ">epoch=176,  error=0.616\n",
      ">epoch=177,  error=0.616\n",
      ">epoch=178,  error=0.615\n",
      ">epoch=179,  error=0.615\n",
      ">epoch=180,  error=0.614\n",
      ">epoch=181,  error=0.614\n",
      ">epoch=182,  error=0.613\n",
      ">epoch=183,  error=0.613\n",
      ">epoch=184,  error=0.612\n",
      ">epoch=185,  error=0.612\n",
      ">epoch=186,  error=0.611\n",
      ">epoch=187,  error=0.611\n",
      ">epoch=188,  error=0.610\n",
      ">epoch=189,  error=0.609\n",
      ">epoch=190,  error=0.609\n",
      ">epoch=191,  error=0.608\n",
      ">epoch=192,  error=0.608\n",
      ">epoch=193,  error=0.607\n",
      ">epoch=194,  error=0.607\n",
      ">epoch=195,  error=0.606\n",
      ">epoch=196,  error=0.606\n",
      ">epoch=197,  error=0.605\n",
      ">epoch=198,  error=0.605\n",
      ">epoch=199,  error=0.604\n",
      ">epoch=200,  error=0.604\n",
      ">epoch=201,  error=0.603\n",
      ">epoch=202,  error=0.603\n",
      ">epoch=203,  error=0.602\n",
      ">epoch=204,  error=0.602\n",
      ">epoch=205,  error=0.601\n",
      ">epoch=206,  error=0.601\n",
      ">epoch=207,  error=0.600\n",
      ">epoch=208,  error=0.600\n",
      ">epoch=209,  error=0.599\n",
      ">epoch=210,  error=0.598\n",
      ">epoch=211,  error=0.598\n",
      ">epoch=212,  error=0.597\n",
      ">epoch=213,  error=0.597\n",
      ">epoch=214,  error=0.596\n",
      ">epoch=215,  error=0.596\n",
      ">epoch=216,  error=0.595\n",
      ">epoch=217,  error=0.595\n",
      ">epoch=218,  error=0.594\n",
      ">epoch=219,  error=0.594\n",
      ">epoch=220,  error=0.593\n",
      ">epoch=221,  error=0.593\n",
      ">epoch=222,  error=0.592\n",
      ">epoch=223,  error=0.591\n",
      ">epoch=224,  error=0.591\n",
      ">epoch=225,  error=0.590\n",
      ">epoch=226,  error=0.590\n",
      ">epoch=227,  error=0.589\n",
      ">epoch=228,  error=0.589\n",
      ">epoch=229,  error=0.588\n",
      ">epoch=230,  error=0.588\n",
      ">epoch=231,  error=0.587\n",
      ">epoch=232,  error=0.587\n",
      ">epoch=233,  error=0.586\n",
      ">epoch=234,  error=0.586\n",
      ">epoch=235,  error=0.585\n",
      ">epoch=236,  error=0.584\n",
      ">epoch=237,  error=0.584\n",
      ">epoch=238,  error=0.583\n",
      ">epoch=239,  error=0.583\n",
      ">epoch=240,  error=0.582\n",
      ">epoch=241,  error=0.582\n",
      ">epoch=242,  error=0.581\n",
      ">epoch=243,  error=0.581\n",
      ">epoch=244,  error=0.580\n",
      ">epoch=245,  error=0.579\n",
      ">epoch=246,  error=0.579\n",
      ">epoch=247,  error=0.578\n",
      ">epoch=248,  error=0.578\n",
      ">epoch=249,  error=0.577\n",
      ">epoch=250,  error=0.577\n",
      ">epoch=251,  error=0.576\n",
      ">epoch=252,  error=0.576\n",
      ">epoch=253,  error=0.575\n",
      ">epoch=254,  error=0.574\n",
      ">epoch=255,  error=0.574\n",
      ">epoch=256,  error=0.573\n",
      ">epoch=257,  error=0.573\n",
      ">epoch=258,  error=0.572\n",
      ">epoch=259,  error=0.572\n",
      ">epoch=260,  error=0.571\n",
      ">epoch=261,  error=0.570\n",
      ">epoch=262,  error=0.570\n",
      ">epoch=263,  error=0.569\n",
      ">epoch=264,  error=0.569\n",
      ">epoch=265,  error=0.568\n",
      ">epoch=266,  error=0.568\n",
      ">epoch=267,  error=0.567\n",
      ">epoch=268,  error=0.566\n",
      ">epoch=269,  error=0.566\n",
      ">epoch=270,  error=0.565\n",
      ">epoch=271,  error=0.565\n",
      ">epoch=272,  error=0.564\n",
      ">epoch=273,  error=0.564\n",
      ">epoch=274,  error=0.563\n",
      ">epoch=275,  error=0.562\n",
      ">epoch=276,  error=0.562\n",
      ">epoch=277,  error=0.561\n",
      ">epoch=278,  error=0.561\n",
      ">epoch=279,  error=0.560\n",
      ">epoch=280,  error=0.559\n",
      ">epoch=281,  error=0.559\n",
      ">epoch=282,  error=0.558\n",
      ">epoch=283,  error=0.558\n",
      ">epoch=284,  error=0.557\n",
      ">epoch=285,  error=0.556\n",
      ">epoch=286,  error=0.556\n",
      ">epoch=287,  error=0.555\n",
      ">epoch=288,  error=0.555\n",
      ">epoch=289,  error=0.554\n",
      ">epoch=290,  error=0.553\n",
      ">epoch=291,  error=0.553\n",
      ">epoch=292,  error=0.552\n",
      ">epoch=293,  error=0.552\n",
      ">epoch=294,  error=0.551\n",
      ">epoch=295,  error=0.550\n",
      ">epoch=296,  error=0.550\n",
      ">epoch=297,  error=0.549\n",
      ">epoch=298,  error=0.549\n",
      ">epoch=299,  error=0.548\n",
      ">epoch=300,  error=0.547\n",
      ">epoch=301,  error=0.547\n",
      ">epoch=302,  error=0.546\n",
      ">epoch=303,  error=0.545\n",
      ">epoch=304,  error=0.545\n",
      ">epoch=305,  error=0.544\n",
      ">epoch=306,  error=0.544\n",
      ">epoch=307,  error=0.543\n",
      ">epoch=308,  error=0.542\n",
      ">epoch=309,  error=0.542\n",
      ">epoch=310,  error=0.541\n",
      ">epoch=311,  error=0.540\n",
      ">epoch=312,  error=0.540\n",
      ">epoch=313,  error=0.539\n",
      ">epoch=314,  error=0.538\n",
      ">epoch=315,  error=0.538\n",
      ">epoch=316,  error=0.537\n",
      ">epoch=317,  error=0.537\n",
      ">epoch=318,  error=0.536\n",
      ">epoch=319,  error=0.535\n",
      ">epoch=320,  error=0.535\n",
      ">epoch=321,  error=0.534\n",
      ">epoch=322,  error=0.533\n",
      ">epoch=323,  error=0.533\n",
      ">epoch=324,  error=0.532\n",
      ">epoch=325,  error=0.531\n",
      ">epoch=326,  error=0.531\n",
      ">epoch=327,  error=0.530\n",
      ">epoch=328,  error=0.529\n",
      ">epoch=329,  error=0.529\n",
      ">epoch=330,  error=0.528\n",
      ">epoch=331,  error=0.527\n",
      ">epoch=332,  error=0.527\n",
      ">epoch=333,  error=0.526\n",
      ">epoch=334,  error=0.525\n",
      ">epoch=335,  error=0.525\n",
      ">epoch=336,  error=0.524\n",
      ">epoch=337,  error=0.523\n",
      ">epoch=338,  error=0.522\n",
      ">epoch=339,  error=0.522\n",
      ">epoch=340,  error=0.521\n",
      ">epoch=341,  error=0.520\n",
      ">epoch=342,  error=0.520\n",
      ">epoch=343,  error=0.519\n",
      ">epoch=344,  error=0.518\n",
      ">epoch=345,  error=0.517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">epoch=346,  error=0.517\n",
      ">epoch=347,  error=0.516\n",
      ">epoch=348,  error=0.515\n",
      ">epoch=349,  error=0.515\n",
      ">epoch=350,  error=0.514\n",
      ">epoch=351,  error=0.513\n",
      ">epoch=352,  error=0.512\n",
      ">epoch=353,  error=0.512\n",
      ">epoch=354,  error=0.511\n",
      ">epoch=355,  error=0.510\n",
      ">epoch=356,  error=0.509\n",
      ">epoch=357,  error=0.509\n",
      ">epoch=358,  error=0.508\n",
      ">epoch=359,  error=0.507\n",
      ">epoch=360,  error=0.506\n",
      ">epoch=361,  error=0.506\n",
      ">epoch=362,  error=0.505\n",
      ">epoch=363,  error=0.504\n",
      ">epoch=364,  error=0.503\n",
      ">epoch=365,  error=0.502\n",
      ">epoch=366,  error=0.502\n",
      ">epoch=367,  error=0.501\n",
      ">epoch=368,  error=0.500\n",
      ">epoch=369,  error=0.499\n",
      ">epoch=370,  error=0.499\n",
      ">epoch=371,  error=0.498\n",
      ">epoch=372,  error=0.497\n",
      ">epoch=373,  error=0.496\n",
      ">epoch=374,  error=0.495\n",
      ">epoch=375,  error=0.494\n",
      ">epoch=376,  error=0.494\n",
      ">epoch=377,  error=0.493\n",
      ">epoch=378,  error=0.492\n",
      ">epoch=379,  error=0.491\n",
      ">epoch=380,  error=0.490\n",
      ">epoch=381,  error=0.489\n",
      ">epoch=382,  error=0.489\n",
      ">epoch=383,  error=0.488\n",
      ">epoch=384,  error=0.487\n",
      ">epoch=385,  error=0.486\n",
      ">epoch=386,  error=0.485\n",
      ">epoch=387,  error=0.484\n",
      ">epoch=388,  error=0.483\n",
      ">epoch=389,  error=0.482\n",
      ">epoch=390,  error=0.482\n",
      ">epoch=391,  error=0.481\n",
      ">epoch=392,  error=0.480\n",
      ">epoch=393,  error=0.479\n",
      ">epoch=394,  error=0.478\n",
      ">epoch=395,  error=0.477\n",
      ">epoch=396,  error=0.476\n",
      ">epoch=397,  error=0.475\n",
      ">epoch=398,  error=0.474\n",
      ">epoch=399,  error=0.473\n",
      ">epoch=400,  error=0.472\n",
      ">epoch=401,  error=0.471\n",
      ">epoch=402,  error=0.470\n",
      ">epoch=403,  error=0.469\n",
      ">epoch=404,  error=0.468\n",
      ">epoch=405,  error=0.467\n",
      ">epoch=406,  error=0.466\n",
      ">epoch=407,  error=0.465\n",
      ">epoch=408,  error=0.464\n",
      ">epoch=409,  error=0.463\n",
      ">epoch=410,  error=0.462\n",
      ">epoch=411,  error=0.461\n",
      ">epoch=412,  error=0.460\n",
      ">epoch=413,  error=0.459\n",
      ">epoch=414,  error=0.458\n",
      ">epoch=415,  error=0.457\n",
      ">epoch=416,  error=0.456\n",
      ">epoch=417,  error=0.455\n",
      ">epoch=418,  error=0.454\n",
      ">epoch=419,  error=0.453\n",
      ">epoch=420,  error=0.452\n",
      ">epoch=421,  error=0.451\n",
      ">epoch=422,  error=0.449\n",
      ">epoch=423,  error=0.448\n",
      ">epoch=424,  error=0.447\n",
      ">epoch=425,  error=0.446\n",
      ">epoch=426,  error=0.445\n",
      ">epoch=427,  error=0.444\n",
      ">epoch=428,  error=0.443\n",
      ">epoch=429,  error=0.441\n",
      ">epoch=430,  error=0.440\n",
      ">epoch=431,  error=0.439\n",
      ">epoch=432,  error=0.438\n",
      ">epoch=433,  error=0.437\n",
      ">epoch=434,  error=0.435\n",
      ">epoch=435,  error=0.434\n",
      ">epoch=436,  error=0.433\n",
      ">epoch=437,  error=0.432\n",
      ">epoch=438,  error=0.430\n",
      ">epoch=439,  error=0.429\n",
      ">epoch=440,  error=0.428\n",
      ">epoch=441,  error=0.427\n",
      ">epoch=442,  error=0.425\n",
      ">epoch=443,  error=0.424\n",
      ">epoch=444,  error=0.423\n",
      ">epoch=445,  error=0.421\n",
      ">epoch=446,  error=0.420\n",
      ">epoch=447,  error=0.419\n",
      ">epoch=448,  error=0.417\n",
      ">epoch=449,  error=0.416\n",
      ">epoch=450,  error=0.415\n",
      ">epoch=451,  error=0.413\n",
      ">epoch=452,  error=0.412\n",
      ">epoch=453,  error=0.410\n",
      ">epoch=454,  error=0.409\n",
      ">epoch=455,  error=0.408\n",
      ">epoch=456,  error=0.406\n",
      ">epoch=457,  error=0.405\n",
      ">epoch=458,  error=0.403\n",
      ">epoch=459,  error=0.402\n",
      ">epoch=460,  error=0.400\n",
      ">epoch=461,  error=0.399\n",
      ">epoch=462,  error=0.397\n",
      ">epoch=463,  error=0.396\n",
      ">epoch=464,  error=0.394\n",
      ">epoch=465,  error=0.393\n",
      ">epoch=466,  error=0.391\n",
      ">epoch=467,  error=0.389\n",
      ">epoch=468,  error=0.388\n",
      ">epoch=469,  error=0.386\n",
      ">epoch=470,  error=0.385\n",
      ">epoch=471,  error=0.383\n",
      ">epoch=472,  error=0.381\n",
      ">epoch=473,  error=0.380\n",
      ">epoch=474,  error=0.378\n",
      ">epoch=475,  error=0.376\n",
      ">epoch=476,  error=0.375\n",
      ">epoch=477,  error=0.373\n",
      ">epoch=478,  error=0.371\n",
      ">epoch=479,  error=0.370\n",
      ">epoch=480,  error=0.368\n",
      ">epoch=481,  error=0.366\n",
      ">epoch=482,  error=0.364\n",
      ">epoch=483,  error=0.363\n",
      ">epoch=484,  error=0.361\n",
      ">epoch=485,  error=0.359\n",
      ">epoch=486,  error=0.357\n",
      ">epoch=487,  error=0.356\n",
      ">epoch=488,  error=0.354\n",
      ">epoch=489,  error=0.352\n",
      ">epoch=490,  error=0.350\n",
      ">epoch=491,  error=0.348\n",
      ">epoch=492,  error=0.346\n",
      ">epoch=493,  error=0.345\n",
      ">epoch=494,  error=0.343\n",
      ">epoch=495,  error=0.341\n",
      ">epoch=496,  error=0.339\n",
      ">epoch=497,  error=0.337\n",
      ">epoch=498,  error=0.335\n",
      ">epoch=499,  error=0.333\n",
      ">epoch=500,  error=0.331\n",
      ">epoch=501,  error=0.329\n",
      ">epoch=502,  error=0.327\n",
      ">epoch=503,  error=0.326\n",
      ">epoch=504,  error=0.324\n",
      ">epoch=505,  error=0.322\n",
      ">epoch=506,  error=0.320\n",
      ">epoch=507,  error=0.318\n",
      ">epoch=508,  error=0.316\n",
      ">epoch=509,  error=0.314\n",
      ">epoch=510,  error=0.312\n",
      ">epoch=511,  error=0.310\n",
      ">epoch=512,  error=0.308\n",
      ">epoch=513,  error=0.306\n",
      ">epoch=514,  error=0.304\n",
      ">epoch=515,  error=0.302\n",
      ">epoch=516,  error=0.300\n",
      ">epoch=517,  error=0.298\n",
      ">epoch=518,  error=0.296\n",
      ">epoch=519,  error=0.294\n",
      ">epoch=520,  error=0.292\n",
      ">epoch=521,  error=0.290\n",
      ">epoch=522,  error=0.288\n",
      ">epoch=523,  error=0.286\n",
      ">epoch=524,  error=0.284\n",
      ">epoch=525,  error=0.282\n",
      ">epoch=526,  error=0.280\n",
      ">epoch=527,  error=0.278\n",
      ">epoch=528,  error=0.276\n",
      ">epoch=529,  error=0.274\n",
      ">epoch=530,  error=0.272\n",
      ">epoch=531,  error=0.270\n",
      ">epoch=532,  error=0.268\n",
      ">epoch=533,  error=0.266\n",
      ">epoch=534,  error=0.264\n",
      ">epoch=535,  error=0.262\n",
      ">epoch=536,  error=0.260\n",
      ">epoch=537,  error=0.258\n",
      ">epoch=538,  error=0.256\n",
      ">epoch=539,  error=0.254\n",
      ">epoch=540,  error=0.252\n",
      ">epoch=541,  error=0.250\n",
      ">epoch=542,  error=0.248\n",
      ">epoch=543,  error=0.246\n",
      ">epoch=544,  error=0.244\n",
      ">epoch=545,  error=0.242\n",
      ">epoch=546,  error=0.240\n",
      ">epoch=547,  error=0.238\n",
      ">epoch=548,  error=0.237\n",
      ">epoch=549,  error=0.235\n",
      ">epoch=550,  error=0.233\n",
      ">epoch=551,  error=0.231\n",
      ">epoch=552,  error=0.229\n",
      ">epoch=553,  error=0.227\n",
      ">epoch=554,  error=0.225\n",
      ">epoch=555,  error=0.224\n",
      ">epoch=556,  error=0.222\n",
      ">epoch=557,  error=0.220\n",
      ">epoch=558,  error=0.218\n",
      ">epoch=559,  error=0.217\n",
      ">epoch=560,  error=0.215\n",
      ">epoch=561,  error=0.213\n",
      ">epoch=562,  error=0.211\n",
      ">epoch=563,  error=0.210\n",
      ">epoch=564,  error=0.208\n",
      ">epoch=565,  error=0.206\n",
      ">epoch=566,  error=0.204\n",
      ">epoch=567,  error=0.203\n",
      ">epoch=568,  error=0.201\n",
      ">epoch=569,  error=0.199\n",
      ">epoch=570,  error=0.198\n",
      ">epoch=571,  error=0.196\n",
      ">epoch=572,  error=0.195\n",
      ">epoch=573,  error=0.193\n",
      ">epoch=574,  error=0.191\n",
      ">epoch=575,  error=0.190\n",
      ">epoch=576,  error=0.188\n",
      ">epoch=577,  error=0.187\n",
      ">epoch=578,  error=0.185\n",
      ">epoch=579,  error=0.184\n",
      ">epoch=580,  error=0.182\n",
      ">epoch=581,  error=0.181\n",
      ">epoch=582,  error=0.179\n",
      ">epoch=583,  error=0.178\n",
      ">epoch=584,  error=0.176\n",
      ">epoch=585,  error=0.175\n",
      ">epoch=586,  error=0.174\n",
      ">epoch=587,  error=0.172\n",
      ">epoch=588,  error=0.171\n",
      ">epoch=589,  error=0.169\n",
      ">epoch=590,  error=0.168\n",
      ">epoch=591,  error=0.167\n",
      ">epoch=592,  error=0.165\n",
      ">epoch=593,  error=0.164\n",
      ">epoch=594,  error=0.163\n",
      ">epoch=595,  error=0.161\n",
      ">epoch=596,  error=0.160\n",
      ">epoch=597,  error=0.159\n",
      ">epoch=598,  error=0.158\n",
      ">epoch=599,  error=0.156\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "train_set = [[1,0.,0],\n",
    "    [1.,0.,1.],\n",
    "    [1, 1.,1.],\n",
    "    [1.,1.,0]]\n",
    "targets = [0, 1, 1, 1]\n",
    "network = initialize_network()\n",
    "epoch_error = train_network(network=network, train=train_set,\n",
    "                            targets=targets,\n",
    "                            l_rate=0.5, n_epoch=600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epoch_error)\n",
    "plt.ylabel('square error')\n",
    "plt.xlabel('epoch')\n",
    "plt.title('Square error per Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction with the trained  network\n",
    "def predict(network, inputs):\n",
    "    outputs = forward_propagate(network, inputs)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0.0, 0] 0.32333241162106896\n",
      "[1.0, 0.0, 1.0] 0.8655849465367054\n",
      "[1, 1.0, 1.0] 0.8983473668822205\n",
      "[1.0, 1.0, 0] 0.858770021234528\n"
     ]
    }
   ],
   "source": [
    "for inputs in train_set:\n",
    "    prediction = predict(network, inputs)\n",
    "    print(inputs,prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">### Remarks on weight update\n",
    "There are few ways to update the weights. \n",
    "* Stochastic Gradient Descent\n",
    "In Stochastic gradient descent, often abbreviated as SGD, the error is calculated , and the weights are updated for each example in the training dataset.<br>\n",
    "I used this method here, since its clear and ilustrative, and the toy data set,is very small.\n",
    "* Batch Gradient Descent\n",
    "In Batch gradient descent the error is calculated for each example in the training dataset, but the weights are updated after a pass on all training examples.\n",
    "* Mini-Batch Gradient Descent\n",
    "In Mini-batch gradient descent we split the training dataset into small batches. The error is calculated ,as a sum or a mean, for each mini batch, and weights are updated after each minibatch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, to build practical neural networks for real problems, \n",
    "we better off using deep learning library.\n",
    "Keras is great library to start with. An ilstrutive example in the next part.\n",
    "* https://keras.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn more:\n",
    "* This part was written as an attempt to understand Hinton's lecture: https://www.coursera.org/learn/neural-networks/lecture/bD3OB/learning-the-weights-of-a-linear-neuron-12-min\n",
    "* Google machine learning course https://developers.google.com/machine-learning/crash-course/reducing-loss/gradient-descent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
